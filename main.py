# -*- coding: utf-8 -*-
"""Copy of master & data clean

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K4q2QGiifYWykSqkxT5PxJu07Cr8j7M3

# Install Peripheral
"""

! pip install -q kaggle
from google.colab import files
import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
from io import StringIO
import zipfile
from sklearn.model_selection import train_test_split
import string
import re

from google.colab import drive
from google.colab import files
drive.mount('/content/drive', force_remount=True)

"""# Insert Master"""

master = pd.read_excel('/content/drive/MyDrive/project_alamat/master.xlsx')

master

master.isnull().sum()

master_lower = master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
master_lower

master_clean = master_lower.reindex(columns=['Subdistrict Code', 'Subdistrict Name', 'City Name', 'City Code', 'Province Name', 'Province Code'])
master_clean = master_clean.rename(columns={'Subdistrict Code': 'subdistrict_code', 'Subdistrict Name': 'subdistrict', 'City Name': 'city', 'City Code': 'city_code', 'Province Name': 'province_name', 'Province Code': 'province_code'})
master_clean

# Mendefinisikan fungsi untuk menggandakan setiap nilai unik sebanyak 10 kali
def expand_unique_values(master_clean):
    # Membuat list kosong untuk menampung hasil
    expanded_rows = []

    # Menggandakan setiap nilai unik sebanyak n kali
    for _, row in master_clean.iterrows():
        for _ in range(600):
            expanded_rows.append(row)

    # Membuat DataFrame baru dari list yang diperluas
    expanded_df = pd.DataFrame(expanded_rows, columns=master_clean.columns)

    return expanded_df

# Memanggil fungsi untuk menggandakan nilai unik
df_expanded = expand_unique_values(master_clean)

# Menampilkan DataFrame hasil
df_expanded

df_expanded.to_csv('/content/drive/MyDrive/project_alamat/master_600.csv',sep=";")

df_expanded

"""# Insert Core Data"""

!unzip "/content/drive/MyDrive/project_alamat/AddressUni.csv.zip"
df = pd.read_csv("AddressUni.csv",delimiter=";")
df.drop('Unnamed: 0', axis=1, inplace=True)
df.head()

df.isnull().sum()

df['PostalCode'] = df['PostalCode'].fillna(0)
df['Subdistrict'] = df['Subdistrict'].fillna("Unknown")
df['StateProvince'] = df['StateProvince'].fillna("Unknown")

df_rename=df.columns = ['address', 'city', 'subdistrict', 'province_name','postal_code', 'subdistrict_code']
df_rename

df_rename = df.reindex(columns=['subdistrict_code', 'subdistrict', 'city', 'province_name', 'address','postal_code'])
df_rename

! pip install -q kaggle

files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download "adjiepamungkas/address-data"

!unzip address-data.zip

data_new = pd.read_csv('shipdeo.user_address_backup.csv',on_bad_lines='skip')

# masukan data ter update
# data_new = pd.read_csv('/content/drive/MyDrive/project_alamat/shipdeo.user_address_backup.csv', on_bad_lines='skip')

"""# Cleaning Data I"""

data_new.drop('_id', axis=1, inplace=True)
# data_new

data_new_rename = data_new.rename(columns={'province': 'province_name', 'postl_code': 'postal_code', 'subsdistrict' : 'subdistrict'})
# data_new_rename

data_new_reindex = data_new_rename.reindex(columns=['subdistrict_code', 'subdistrict', 'city', 'province_name', 'address','postal_code'])
# data_new_reindex

data_full = pd.concat([df_rename, data_new_reindex])

data_full

data_full = data_full.drop_duplicates(subset='address')

data_full = data_full[data_full['address'].str.len() >= 5]

data_full

data_full.to_csv('/content/drive/MyDrive/project_alamat/data_clean_I.csv',sep=";")

"""# Cleaning Data II"""

data_full = pd.read_csv('/content/drive/MyDrive/project_alamat/data_clean_I.csv',sep=";")
data_full.drop('Unnamed: 0', axis=1, inplace=True)

data_full = data_full.dropna(subset=['address'])

data_full

data_full = data_full.applymap(lambda x: x.lower() if isinstance(x, str) else x)
# df_clean_lower

data_full.to_csv('/content/drive/MyDrive/project_alamat/data_clean_II.csv',sep=";")

"""# Cleaning Data III"""

df_clean_lower = pd.read_csv('/content/drive/MyDrive/project_alamat/data_clean_II.csv',sep=";")
df_clean_lower.drop('Unnamed: 0', axis=1, inplace=True)
df_clean_lower

# Fungsi untuk menghapus tanda baca
def remove_punctuation(text):
    return text.translate(str.maketrans("", "", string.punctuation))

df_clean_lower['address'] = df_clean_lower['address'].apply(remove_punctuation)

# Fungsi untuk menghapus spasi di awal dan akhir teks
def remove_whitespace_LT(series):
    return series.apply(lambda x: x.strip() if isinstance(x, str) else x)

columns_to_clean = ['subdistrict', 'address']
df_clean_lower[columns_to_clean] = df_clean_lower[columns_to_clean].apply(remove_whitespace_LT)

# Fungsi untuk mengganti beberapa spasi menjadi satu spasi
def remove_whitespace_multiple(text):
    return re.sub('\s+', ' ', text)

df_clean_lower['address'] = df_clean_lower['address'].apply(remove_whitespace_multiple)

# Fungsi untuk menghapus teks yang mengandung angka dengan panjang 8 hingga 13 digit
def remove_text_with_digits(text):
    if re.search(r'\b\d{8,13}\b', text):
        return ''
    return text

df_clean_lower['address'] = df_clean_lower['address'].apply(remove_text_with_digits)

df_clean_lower

df_clean_lower.to_csv('/content/drive/MyDrive/project_alamat/data_clean_final.csv',sep=";")

"""# Validate Data"""

df = pd.read_csv('/content/drive/MyDrive/project_alamat/data_clean_final.csv',sep=";")
df.drop('Unnamed: 0', axis=1, inplace=True)
df

df = df.dropna(subset=['address'])

df.isnull().sum()

df

# df_clean_lower = df_clean_lower.dropna(subset=['address'])

# filtered_df = df.loc[df['subdistrict'] == 'ILIR BARAT II']
# print(filtered_df)

jumlah_kecamatan= (df['subdistrict'] == 'lengkong').sum()
print(jumlah_kecamatan)

# dataz = df_filtered.groupby('city', group_keys=False).apply(lambda data: data.sample(frac=.7, replace=True, random_state=1))

# dataz

# Fungsi untuk menyisakan N record per kelompok
def keep_top_n_per_group(df, group_columns, n=400):
    # Grouping by the specified columns
    grouped = df.groupby(group_columns)

    # Taking the top N records from each group
    top_n = grouped.head(n)

    return top_n

# Menggunakan fungsi untuk menyisakan n record per kombinasi kecamatan dan kota
data_reduced = keep_top_n_per_group(df, ['subdistrict', 'city'], 400)

# Menampilkan hasil
data_reduced

# data_reduced.to_csv('/content/drive/MyDrive/project_alamat/data_for_train_300.csv',sep=";")

print('kecamatan sesuai n \n',data_reduced['subdistrict'].value_counts()[data_reduced['subdistrict'].value_counts() == 401])

df = data_reduced.copy()

df.head()

data_reduced.to_csv('/content/drive/MyDrive/project_alamat/data_clean_400.csv',sep=";")

data_reduced.isnull().sum()

"""# JOIN MASTER & DATA"""

data = pd.read_csv('/content/drive/MyDrive/project_alamat/data_clean_300.csv',sep=";")

master = pd.read_csv('/content/drive/MyDrive/project_alamat/master_300.csv',sep=";")

data

master

data.drop('Unnamed: 0', axis=1, inplace=True)
master.drop('Unnamed: 0', axis=1, inplace=True)

data

master

print(data.dtypes)
print(master.dtypes)

result = pd.merge(master, data, how='left', on=['subdistrict_code', 'subdistrict'])

result

"""# CODEXED"""

# masukan data hasil join
datas = pd.read_csv('/content/drive/MyDrive/project_alamat/joined_300.csv',on_bad_lines='skip',
                     encoding='utf-8', delimiter=';', engine='python')

# datas.drop('Unnamed: 0', axis=1, inplace=True)

datas

datas = datas.drop('subdistrict', axis=1)

# prompt: change name column subdistrict_new to subdistrict

datas.rename(columns={'subdistrict_new': 'subdistrict'}, inplace=True)

datas = datas.reindex(columns=['subdistrict_code', 'subdistrict', 'city', 'province_name', 'address','postal_code'])

datas

datas['subdistrict'] = datas['subdistrict'].astype(str).str.replace(r'_\d+', '', regex=True) # Cast the column to string type first

# datas = datas.dropna(subset=['address'])

datas

# datas['address'] = datas['address'].fillna(' ')
# datas['postal_code'].fillna(0, inplace=True)

# from sklearn.model_selection import train_test_split
# import string
# import re

# def remove_punctuation(text):
#     return text.translate(str.maketrans("","",string.punctuation))

# datas['address'] = datas['address'].apply(remove_punctuation)

# #remove whitespace leading & trailing
# def remove_whitespace_LT(text):
#     return text.strip()

# datas['address'] = datas['address'].apply(remove_whitespace_LT)

# #remove multiple whitespace into single whitespace
# def remove_whitespace_multiple(text):
#     return re.sub('\s+',' ',text)

# datas['address'] = datas['address'].apply(remove_whitespace_multiple)

datas

nama_kecamatan = ['lengkong','tarogong kidul','gambir','bogor utara','pangandaran','pademangan', 'kaliwates', 'pasar kliwon', 'gondomanan','cilacap tengah','tambaksari', 'klojen', 'gedangan', 'pamekasan', 'sampang', 'marpoyan damai','way halim', 'baiturrahman','medan selayang','ilir barat ii','padang barat', 'balikpapan selatan','banjarmasin utara','samarinda kota', 'bontang utara', 'panakkukang', 'kota tengah', 'malalayang', 'jayapura utara','ampenan']

# Menghitung jumlah masing-masing nama kecamatan dalam daftar
jumlah_per_kecamatan = datas['subdistrict'].value_counts().loc[nama_kecamatan]
print(jumlah_per_kecamatan)

# Memfilter DataFrame untuk hanya menyertakan nama kecamatan yang ada dalam daftar
filtered_df = datas[datas['subdistrict'].isin(nama_kecamatan)]

# Menghitung jumlah nilai null dalam kolom 'kecamatan' untuk setiap nama kecamatan
jumlah_null_per_kecamatan = datas['subdistrict'].isnull().groupby(datas['subdistrict']).sum()
print(jumlah_null_per_kecamatan)

jumlah_kecamatan_a = (datas['subdistrict'] == 'ilir barat ii').sum()
print(jumlah_kecamatan_a)

filtered_df = datas.loc[(datas['subdistrict'] == 'lengkong') & (datas['city'] == 'kota bandung')]
filtered_df

nan_per_column = datas.isna().sum()

# cek sebaran data
print(df['subdistrict'].value_counts()[df['subdistrict'].value_counts() == 300])

codex = datas.copy()
codex.head()

codex['Codex'] = pd.factorize(codex['subdistrict_code'])[0]

codex.drop_duplicates(['Codex'],inplace=True)

codex

codex.drop(['postal_code', 'address'], axis=1, inplace=True)

# New column order
new_order = ['city', 'subdistrict', 'province_name', 'subdistrict_code','Codex']

# Reindex columns
codex = codex.reindex(columns=new_order)

codex

codex.to_csv('/content/drive/MyDrive/Colab Notebooks/AddressToCodex_300.csv',sep=";")

df['Codex'] = pd.factorize(df['subdistrict_code'])[0]

df

data = df[['address','Codex']].copy()

data

uniqueValues = data['Codex'].nunique()
print('Unique elements in column "Codex" ')
print(uniqueValues)

sentences = data['address'].values
y = data['Codex'].values

data.to_csv('/content/drive/MyDrive/Colab Notebooks/data_train_500.csv',sep=";")

"""# TRAIN MODEL"""

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data_train_300.csv',sep=";")

df

df = df.drop('Unnamed: 0', axis=1)

df = df.fillna(' ')

from sklearn.model_selection import train_test_split
import string
import re

sentences = df['address'].values
y = df['Codex'].values

# print("Max value in y:", max(y))
  # print("Min value in y:", min(y))

# num_classes = max(y) + 1

df.isnull().sum()

# df = df.dropna()

df

uniqueValues = df['Codex'].nunique()
print('Unique elements in column "Codex" ')
print(uniqueValues)

sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.1,random_state=7)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from sklearn.feature_extraction.text import CountVectorizer
from keras.models import Sequential
from keras import layers
from tensorflow import keras
import joblib

vectorizer = CountVectorizer(ngram_range=(1, 2))
vectorizer.fit(sentences_train)

X_train = vectorizer.transform(sentences_train)
X_test = vectorizer.transform(sentences_test)
X_train

input_dim = X_train.shape[1]
print(input_dim)

# from sklearn.preprocessing import LabelEncoder

# le = LabelEncoder()
# y_train = le.fit_transform(y_train)
# y_test = le.transform(y_test)

y

joblib.dump(vectorizer, "/content/drive/MyDrive/Colab Notebooks/vectorizer_300.pkl")

vectorizer = joblib.load("/content/drive/MyDrive/Colab Notebooks/vectorizer_300.pkl")

# tokenizer = Tokenizer(num_words=200000, oov_token='x')
# tokenizer.fit_on_texts(sentences_train)
# tokenizer.fit_on_texts(sentences_test)

# sekuens_train = tokenizer.texts_to_sequences(sentences_train)
# sekuens_test = tokenizer.texts_to_sequences(sentences_test)
# padded_train = pad_sequences(sekuens_train)
# padded_test = pad_sequences(sekuens_test)

# num_classes = len(le.classes_)
# print(num_classes)

# model = tf.keras.Sequential([
# tf.keras.layers.Embedding(input_dim=200000, output_dim=32),
# tf.keras.layers.LSTM(128),
# tf.keras.layers.Dense(256, activation='relu'),
# tf.keras.layers.Dense(64, activation='relu'),
# tf.keras.layers.Dropout(0.5),
# tf.keras.layers.Dense(7001, activation='sigmoid')])

# model.compile(
# optimizer = tf.optimizers.Adam(),
# loss = 'sparse_categorical_crossentropy',
# metrics = ['accuracy'])
# model.summary()

# model.summary()

# history = model.fit(
# padded_train,
# y_train,
# validation_data = (padded_test, y_test),
# epochs = 10,
# verbose = 1,
# batch_size=64)

model = Sequential()
model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))
model.add(layers.Dense(uniqueValues, activation='sigmoid'))

model.compile(
optimizer = tf.optimizers.Adam(),
loss = 'mean_squared_error',
metrics = ['accuracy'])
model.summary()

history = model.fit(X_train, y_train,
epochs=10,
verbose=1,
validation_data=(X_test, y_test),
batch_size=256)

model.save("/content/drive/MyDrive/Colab Notebooks/DeepLearning_300.h5")

# d=1

# model.compile(
# optimizer = tf.optimizers.Adam(),
# loss = 'sparse_categorical_crossentropy',
# metrics = ['accuracy'])
# model.summary()

"""# EVALUATE MODEL"""

from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

import tensorflow as tf
import numpy as np
from tensorflow.keras import mixed_precision

import joblib
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras # Import keras from tensorflow
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/DeepLearning_300.h5')
vectorizer = joblib.load("/content/drive/MyDrive/Colab Notebooks/vectorizer_300.pkl")

data_test  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data_train_300.csv',sep=";")
# data_test = data_test.dropna(subset=['address', 'Codex'])

sentences_test = data_test['address'].astype(str).values
X_test = vectorizer.transform(sentences_test)

# Mengubah labels_test menjadi bentuk numerik
y_test = data_test['Codex'].values
label_encoder = LabelEncoder()
y_test = label_encoder.fit_transform(y_test)
y_test = to_categorical(y_test)

# Membuat prediksi dalam batch untuk menghindari OOM
batch_size = 32
num_batches = int(np.ceil(X_test.shape[0] / batch_size))

y_pred = []
for i in range(num_batches):
    start_idx = i * batch_size
    end_idx = (i + 1) * batch_size
    y_pred_batch = model.predict(X_test[start_idx:end_idx])
    y_pred.append(y_pred_batch)

y_pred = np.vstack(y_pred)

"""# USE OLD DATA"""

datas = pd.read_csv('/content/drive/MyDrive/project_alamat/AddressUse.csv',sep=";")

datas

datas = datas.drop('Unnamed: 0', axis=1)

datas['full_lower'] = datas['full_lower'].fillna(' ')

def remove_punctuation(text):
    return text.translate(str.maketrans("","",string.punctuation))

datas['full_lower'] = datas['full_lower'].apply(remove_punctuation)

#remove whitespace leading & trailing
def remove_whitespace_LT(text):
    return text.strip()

datas['full_lower'] = datas['full_lower'].apply(remove_whitespace_LT)

#remove multiple whitespace into single whitespace
def remove_whitespace_multiple(text):
    return re.sub('\s+',' ',text)

datas['full_lower'] = datas['full_lower'].apply(remove_whitespace_multiple)

sentences = datas['full_lower'].values
y = datas['Codex'].values

# datas = datas.dropna()

datas.isnull().sum()

# datas['full_lower'] = datas['full_lower'].astype(str)

sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.1,random_state=7)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from sklearn.feature_extraction.text import CountVectorizer
from keras.models import Sequential
from keras import layers
from tensorflow import keras
import joblib

vectorizer = CountVectorizer(ngram_range=(1, 2))
vectorizer.fit(sentences_train)

X_train = vectorizer.transform(sentences_train)
X_test  = vectorizer.transform(sentences_test)
X_train

input_dim = X_train.shape[1]  # Number of features

y

# example for saving python object as pkl
joblib.dump(vectorizer, "/content/drive/MyDrive/project_alamat/vectorizer_old.pkl")

# loading pickled vectorizer
vectorizer = joblib.load("/content/drive/MyDrive/project_alamat/vectorizer_old.pkl")

# tokenizer = Tokenizer(num_words=200000, oov_token='x')
# tokenizer.fit_on_texts(sentences_train)
# tokenizer.fit_on_texts(sentences_test)

# sekuens_train = tokenizer.texts_to_sequences(sentences_train)
# sekuens_test = tokenizer.texts_to_sequences(sentences_test)

# padded_train = pad_sequences(sekuens_train)
# padded_test = pad_sequences(sekuens_test)

# model = tf.keras.Sequential([
#     tf.keras.layers.Embedding(input_dim=200000, output_dim=32),
#     tf.keras.layers.LSTM(128),
#     tf.keras.layers.Dense(256, activation='relu'),
#     tf.keras.layers.Dense(64, activation='relu'),
#     tf.keras.layers.Dropout(0.5),
#     tf.keras.layers.Dense(6594, activation='sigmoid')])

# model.compile(
#     optimizer = tf.optimizers.Adam(),
#     loss = 'sparse_categorical_crossentropy',
#     metrics = ['accuracy'])
# model.summary()

# history = model.fit(
#     padded_train,
#     y_train,
#     validation_data = (padded_test, y_test),
#     epochs = 10,
#     verbose = 1,
#     batch_size=64)

model = Sequential()
model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))
model.add(layers.Dense(6607, activation='sigmoid'))

model.compile(
    optimizer = tf.optimizers.Adam(),
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy'])
model.summary()

history = model.fit(X_train, y_train,
                    epochs=10,
                    verbose=1,
                    validation_data=(X_test, y_test),
                    batch_size=256)

model.save("/content/drive/MyDrive/project_alamat/DeepLearning.h5")

"""# New Section"""

model = keras.models.load_model('/content/drive/MyDrive/project_alamat/DeepLearning.h5')

vectorizer = joblib.load("/content/drive/MyDrive/project_alamat/vectorizer.pkl")

model.compile(
    optimizer = tf.optimizers.Adam(),
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy'])
model.summary()

# Predicting the Test set results
naddress=["buah batu"]
naddresss = vectorizer.transform(naddress).toarray()
prediction=model.predict(naddresss)

classes = np.argmax(prediction, axis = 1)
classes = (int(classes))
datas[datas['Codex'] == classes]

classess = np.argsort(np.max(prediction, axis=0))[-2]
classess = (int(classess))
datas[datas['Codex'] == classess]

y = np.sum(prediction)

probabilities = model.predict_prob(naddresss)